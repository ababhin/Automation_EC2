---
- name: Hadoop Cluster Setup
  hosts: all
  become: true

  vars:
    hadoop_version: "3.3.6"
    hadoop_download_url: "https://downloads.apache.org/hadoop/common/hadoop-{{ hadoop_version }}/hadoop-{{ hadoop_version }}.tar.gz"
    hadoop_install_dir: "/usr/local"
    hadoop_tmp_dir: "/var/tmp"
    hadoop_user: "hadoop"
    hadoop_data_dirs:
      - "{{ hadoop_install_dir }}/hadoop_data/hdfs/namenode"
      - "{{ hadoop_install_dir }}/hadoop_data/hdfs/datanode"
    hadoop_env_vars:
      - 'export HADOOP_HOME={{ hadoop_install_dir }}/hadoop'
      - 'export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin'
      - 'export JAVA_HOME=/usr/lib/jvm/java-11-amazon-corretto'
      - 'export HDFS_NAMENODE_USER={{ hadoop_user }}'
      - 'export HDFS_DATANODE_USER={{ hadoop_user }}'
      - 'export HDFS_SECONDARYNAMENODE_USER={{ hadoop_user }}'
      - 'export YARN_RESOURCEMANAGER_USER={{ hadoop_user }}'
      - 'export YARN_NODEMANAGER_USER={{ hadoop_user }}'

  tasks:
    - name: Install Java on Amazon Linux
      shell: |
        amazon-linux-extras enable corretto11
        yum install -y java-11-amazon-corretto
      when: ansible_distribution == "Amazon"

    - name: Ensure Hadoop temp directory exists
      file:
        path: "{{ hadoop_tmp_dir }}"
        state: directory
        owner: root
        group: root
        mode: '1777'

    - name: Download Hadoop
      get_url:
        url: "{{ hadoop_download_url }}"
        dest: "{{ hadoop_tmp_dir }}/hadoop.tar.gz"

    - name: Extract Hadoop
      unarchive:
        src: "{{ hadoop_tmp_dir }}/hadoop.tar.gz"
        dest: "{{ hadoop_install_dir }}"
        remote_src: yes

    - name: Create Hadoop symlink
      file:
        src: "{{ hadoop_install_dir }}/hadoop-{{ hadoop_version }}"
        dest: "{{ hadoop_install_dir }}/hadoop"
        state: link

    - name: Configure Hadoop environment variables
      lineinfile:
        path: /etc/profile.d/hadoop.sh
        create: yes
        line: "{{ item }}"
      with_items: "{{ hadoop_env_vars }}"

    - name: Source Hadoop environment variables
      shell: |
        source /etc/profile.d/hadoop.sh
        echo $PATH
      args:
        executable: /bin/bash

    - name: Copy Hadoop Configuration Files
      template:
        src: templates/{{ item }}
        dest: "{{ hadoop_install_dir }}/hadoop/etc/hadoop/{{ item }}"
      with_items:
        - core-site.xml
        - hdfs-site.xml
        - yarn-site.xml
        - mapred-site.xml

    - name: Create Hadoop user
      user:
        name: "{{ hadoop_user }}"
        shell: /bin/bash
        create_home: yes
        home: "/home/{{ hadoop_user }}"

    - name: Create Hadoop directories for HDFS
      file:
        path: "{{ item }}"
        state: directory
        owner: "{{ hadoop_user }}"
        group: "{{ hadoop_user }}"
        mode: '0755'
      with_items: "{{ hadoop_data_dirs }}"

    - name: Ensure Hadoop logs directory exists
      file:
        path: "{{ hadoop_install_dir }}/hadoop/logs"
        state: directory
        owner: "{{ hadoop_user }}"
        group: "{{ hadoop_user }}"
        mode: '0755'

    - name: Format HDFS (NameNode only)
      become: true
      become_user: "{{ hadoop_user }}"
      shell: |
        source /etc/profile.d/hadoop.sh
        hdfs namenode -format
      args:
        executable: /bin/bash
      when: "'namenode' in group_names"

    - name: Start Hadoop Services (NameNode)
      become: true
      become_user: "{{ hadoop_user }}"
      shell: |
        source /etc/profile.d/hadoop.sh
        start-dfs.sh
        start-yarn.sh
      args:
        executable: /bin/bash
      when: "'namenode' in group_names"

    - name: Start DataNode Services
      become: true
      become_user: "{{ hadoop_user }}"
      shell: |
        source /etc/profile.d/hadoop.sh
        start-dfs.sh
      args:
        executable: /bin/bash
      when: "'datanodes' in group_names"

    - name: Clean up temporary files
      file:
        path: "{{ hadoop_tmp_dir }}/hadoop.tar.gz"
        state: absent
